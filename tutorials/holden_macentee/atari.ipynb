{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import gymnasium as gym\n",
    "import time\n",
    "import copy\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import cv2\n",
    "from time import sleep\n",
    "sleep(0.0416)\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "from IPython import display\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import rescale\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from collections import deque, namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    # If frame is a tuple, extract the first element (assuming it's the image)\n",
    "    if isinstance(frame, tuple):\n",
    "        frame = frame[0]  # Adjust the index based on the structure\n",
    "    \n",
    "    # Convert to grayscale and rescale\n",
    "    gray = rgb2gray(frame)\n",
    "    scaled = rescale(gray, 0.5, anti_aliasing=True)\n",
    "    return scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        # self.fc = nn.Sequential(\n",
    "        #     nn.Linear(64 * 7 * 7, 512),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(512, n_actions)\n",
    "        # )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(3456, 512),  # Adjust input size to match flattened convolutional output\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "    \n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.99\n",
    "EPSILON = 0.05  # Exploration probability\n",
    "TARGET_UPDATE = 10  # How often to update the target network\n",
    "MEMORY_CAPACITY = 10000\n",
    "\n",
    "# Setup the Atari environment\n",
    "env = gym.make('Pong-v4', render_mode=\"human\")\n",
    "n_actions = env.action_space.n\n",
    "print(env.unwrapped.get_action_meanings())\n",
    "\n",
    "# Create networks\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "policy_net = DQN((1, 84, 84), n_actions).to(device)\n",
    "target_net = DQN((1, 84, 84), n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "# Replay memory\n",
    "memory = ReplayMemory(MEMORY_CAPACITY)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(policy_net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class PongEnv:\n",
    "    def __init__(self):\n",
    "        # Initialize your environment, paddles, ball, etc.\n",
    "        self.reset()  # Initialize the state\n",
    "        \n",
    "    def reset(self):\n",
    "        # Reset the environment to the initial state\n",
    "        self.ball_position = np.array([0.0, 0.0])  # Example initial position\n",
    "        self.paddle_position = np.array([0.0, 0.0])\n",
    "        self.done = False\n",
    "        return self.get_state()  # Return the initial state\n",
    "\n",
    "    def get_state(self):\n",
    "        # Return the current state of the game (e.g., positions of ball and paddles)\n",
    "        return np.concatenate((self.ball_position, self.paddle_position))\n",
    "\n",
    "    def step(self, action):\n",
    "        # Update the game state based on the action\n",
    "        self.update_game_state(action)\n",
    "        \n",
    "        # Check the ball position to determine the reward\n",
    "        ball_x = self.ball_position[0]\n",
    "        reward = 0\n",
    "        \n",
    "        if ball_x < -1:  # Assuming -1 is the left boundary\n",
    "            reward = -1  # Enemy scores\n",
    "            self.done = True  # Episode ends\n",
    "        elif ball_x > 1:  # Assuming 1 is the right boundary\n",
    "            reward = 1  # Agent scores\n",
    "            self.done = True  # Episode ends\n",
    "        else:\n",
    "            # Reward for hitting the ball or no change\n",
    "            reward = 0  # Could be modified for hitting logic\n",
    "        \n",
    "        return self.get_state(), reward, self.done, {}  # Return next state, reward, done flag, info\n",
    "\n",
    "    def update_game_state(self, action):\n",
    "        # Implement logic to update ball and paddle positions based on action\n",
    "        # Example:\n",
    "        if action == 0:  # Move paddle up\n",
    "            self.paddle_position[1] += 1  # Update paddle position\n",
    "        elif action == 1:  # Move paddle down\n",
    "            self.paddle_position[1] -= 1  # Update paddle position\n",
    "        \n",
    "        # Update the ball position (dummy example)\n",
    "        self.ball_position[0] += 0.1  # Move ball to the right for simplicity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state, steps_done):\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPSILON\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    next_state_batch = torch.cat(batch.next_state)\n",
    "\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    next_state_values = target_net(next_state_batch).max(1)[0].detach()\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    loss = nn.SmoothL1Loss()(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_function(observation, action, next_observation):\n",
    "    reward = 0\n",
    "\n",
    "    # Example reward structure based on game events\n",
    "    if next_observation['score'] > observation['score']:\n",
    "        reward += 1  # Positive reward for increasing the score\n",
    "    elif next_observation['lives'] < observation['lives']:\n",
    "        reward -= 1  # Negative reward for losing a life\n",
    "    elif action == \"shoot\" and next_observation['enemy_defeated']:\n",
    "        reward += 5  # Reward for defeating an enemy\n",
    "    \n",
    "    return reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\macenh\\Documents\\RCOS\\ReinforcementLearning\\MaxCut_RL\\tutorials\\holden_macentee\\.venv\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:335: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m env\u001b[38;5;241m.\u001b[39mrender()\n\u001b[0;32m      9\u001b[0m action \u001b[38;5;241m=\u001b[39m select_action(state, t)\n\u001b[1;32m---> 10\u001b[0m next_state, reward, done, _, _\u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#print(reward)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#cv2.imshow(\"Game\", next_state)\u001b[39;00m\n\u001b[0;32m     15\u001b[0m next_state \u001b[38;5;241m=\u001b[39m preprocess_frame(next_state)\n",
      "File \u001b[1;32mc:\\Users\\macenh\\Documents\\RCOS\\ReinforcementLearning\\MaxCut_RL\\tutorials\\holden_macentee\\.venv\\lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\macenh\\Documents\\RCOS\\ReinforcementLearning\\MaxCut_RL\\tutorials\\holden_macentee\\.venv\\lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\macenh\\Documents\\RCOS\\ReinforcementLearning\\MaxCut_RL\\tutorials\\holden_macentee\\.venv\\lib\\site-packages\\shimmy\\atari_env.py:294\u001b[0m, in \u001b[0;36mAtariEnv.step\u001b[1;34m(self, action_ind)\u001b[0m\n\u001b[0;32m    292\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(frameskip):\n\u001b[1;32m--> 294\u001b[0m     reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39male\u001b[38;5;241m.\u001b[39mact(action)\n\u001b[0;32m    295\u001b[0m is_terminal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39male\u001b[38;5;241m.\u001b[39mgame_over(with_truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    296\u001b[0m is_truncated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39male\u001b[38;5;241m.\u001b[39mgame_truncated()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_episodes = 50\n",
    "for i_episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    state = preprocess_frame(state)\n",
    "    state = torch.tensor(state, device=device, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    for t in range(10000):\n",
    "        env.render()\n",
    "        action = select_action(state, t)\n",
    "        next_state, reward, done, _, _= env.step(action.item())\n",
    "        print(reward)\n",
    "        #cv2.imshow(\"Game\", next_state)\n",
    "        \n",
    "        \n",
    "        next_state = preprocess_frame(next_state)\n",
    "        next_state = torch.tensor(next_state, device=device, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "\n",
    "\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        optimize_model()\n",
    "\n",
    "        #if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            #break\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
